# -*- coding: utf-8 -*-
"""face-recognition-siamese-keras-4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D0WqzKb2zaXrD0wWr5HazQiSpCPct0f0

# Face Recognition System Using Siamese in Keras
---

## **Connect Google Colab with Google Drive**

---
"""

from google.colab import drive

drive.mount('/content/gdrive')

!ls "/content/gdrive/My Drive/deep_learning/mtcnn_face_recognition"

"""##  Tensorflow with GPU

## Enabling and testing the GPU

First, you'll need to enable GPUs for the notebook:

- Navigate to Editâ†’Notebook Settings
- select GPU from the Hardware Accelerator drop-down

Next, we'll confirm that we can connect to the GPU with tensorflow:
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""### **Train**"""

!pip install mtcnn

# face detection for the 5 Celebrity Faces Dataset
from os import listdir
from os.path import isdir
from PIL import Image
from matplotlib import pyplot
from numpy import savez_compressed
from numpy import asarray
from mtcnn.mtcnn import MTCNN

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding

# Commented out IPython magic to ensure Python compatibility.
# create mtcnn detector
# profile time using %prun
# %prun detector = MTCNN()

"""## Utility Methods"""

from numpy import asarray
from PIL import Image
from matplotlib import pyplot as plt
import os, random
import shutil

def read_image(image_path, method="PIL", rgb=True):
    """
    Read image from disk
    
    Args:
        image_path (str): Image path
        method (str, optional): which library to use. Defaults to "PIL".
        rgb (bool, optional): convert to rgb. Defaults to True.
    
    Returns:
        tuple: image_object, and image pixels in numpy format
    """
    if method == "PIL":
        # load image from file
        image = Image.open(image_path)
        if rgb:
            # convert to RGB, if needed
            image = image.convert('RGB')
        # convert to array
        # obtain_image_pixels
        pixels = asarray(image)
        return (image, pixels)
    elif method == "CV":
        print("Not implimented yet")
        return (False, False)
    elif method == "MATPLOT":
        print("Not implimented yet")
        return (False, False)
    else:
        print("Please select proper reading method")
        return (False, False)

def detect_faces_in_image(image_array, detector):
    """
    Detect faces in image using MTCNN
    
    Args:
        image_array (numpy array): Image pixels
        detector (object): MTCNN face detector
    
    Returns:
        array: Numpy array of detected faces(ndarray)
    """
    faces = detector.detect_faces(image_array)
    return faces

def extract_faces_from_image(image_array, detector, required_size=(160, 160), convert_2_numpy=True):
    """
    Extract faces from image
    
    Args:
        image_array (array): Image pixels in numpy format
        detector (object): face detector object
        required_size (tuple, optional): Final image resolution. Defaults to (160, 160).
        convert_2_numpy (bool, optional): convert pil face image to numpy. Defaults to True.
    
    Returns:
        list: If convert_2_numpy flag set to true then it returns list of faces in numpy format 
              otherwise it returns faces in pillow format
    """
    faces = detector.detect_faces(image_array)

    face_images = []

    for face in faces:
        # extract the bounding box from the first face
        x1, y1, width, height = face['box']
        # bug fix
        x1, y1 = abs(x1), abs(y1)
        x2, y2 = x1 + width, y1 + height
        # extract the face
        face_boundary = image_array[y1:y2, x1:x2]
        # resize pixels to the model size
        face_image = Image.fromarray(face_boundary)
        face_image = face_image.resize(required_size)
        if convert_2_numpy:
            face_array = asarray(face_image)
            face_images.append(face_array)
        else:
            face_images.append(face_image)

    return face_images

def extract_save_face(src, dest):
    """
    Extract face from image and save
    
    Args:
        src (str): Source image file path
        dest (str): Dest image file path
    """
    filename, file_extension = os.path.splitext(src)

    image, pixels = read_image(src)

    faces = extract_faces_from_image(pixels, detector, convert_2_numpy=False)

    if faces:
        img_single_face = faces[0]
        # pillow write image
        img_single_face.save(dest)
        # print(f"saved face from {src}")
        return True
    else:
        print(f"No face found in {src}")
        return False

def recursive_dir(src, dest, task='extract_write_face'):
    """
    Recursively iterate in root input dir
    and clone its structure by modifying files if necessary
    
    Args:
        src (str): Source folder
        dest (str): Destination folder
        task (str, optional): Operation to perform on each file. Defaults to 'extract_write_face'.
                            vales are extract_write_face|face_alignment
        
        Note:
        Pass string name to run function
        use it in try catch
        make a callback function that will modify file
            import foo
            method_to_call = getattr(foo, 'bar')
            result = method_to_call()
            # shorten way
            result = getattr(foo, 'bar')()

        - also add filters or ignores
    """
    if os.path.isdir(src):
        if not os.path.isdir(dest):
            os.makedirs(dest)

        folders = os.listdir(src)

        for folder in folders:
            recursive_dir(
                os.path.join(src, folder), 
                os.path.join(dest, folder),
                task=task
            )
    else:
        if task == 'extract_write_face':
            # extract face and write
            extract_save_face(src, dest)

        elif task == 'face_alignment':
            align_save_face(src, dest)
        else:
            # copy file as it is
            shutil.copyfile(src, dest)


def directory_load_faces(directory, is_extract_face=True):
    """
    load faces from a directory
    
    Args:
        directory (str): Dataset directory to load face data
        is_extract_face (bool, optional): If true extract face from image first and then save. If source image is already cropped face then no need to extract face so set to false. Defaults to True.
    
    Returns:
        list: All faces list
    """
    all_faces = list()
    # enumerate files
    for filename in listdir(directory):
        # path
        try:
            path = os.path.join(directory, filename)
            image, pixels = read_image(path)
            # get face
            if is_extract_face:
                faces = extract_faces_from_image(pixels, detector)
                # store
                all_faces.extend(faces)
            else:
                all_faces.extend(pixels)
        except exception as e:
            print(e)
    return all_faces
 

def load_dataset(directory, is_extract_face=True):
    """
    load a dataset that contains one subdir for each class that in turn contains images
    
    Args:
        directory (str): Directory name with fullpath
        is_extract_face (bool, optional): If true extract face from image first and then save. If source image is already cropped face then no need to extract face so set to false. Defaults to True.
    
    Returns:
        Tuple: Tuple of ndarray dataset face data and labels(taken from folder name)
    """
    X, y = list(), list()
    # enumerate folders, on per class
    for subdir in listdir(directory):
        # path
        path = os.path.join(directory, subdir)
  
        # skip any files that might be in the dir
        if not isdir(path):
            continue
        # load all faces in the subdirectory
        faces = directory_load_faces(path, is_extract_face=is_extract_face)
        # create labels
        labels = [subdir for _ in range(len(faces))]
        # summarize progress
        print('>loaded %d examples for class: %s' % (len(faces), subdir))
        # store
        X.extend(faces)
        y.extend(labels)
    return asarray(X), asarray(y)

import os

root_folder = '/content/gdrive/My Drive/deep_learning/'
dataset_folder = os.path.join(root_folder, "vgg_2_dataset/")

FOLDER_FULL_IMAGES = os.path.join(dataset_folder, "full_images/")
FOLDER_FACE_ONLY = os.path.join(dataset_folder, "face_only/")

IMAGE_WIDTH = 160
IMAGE_HEIGHT = 160
IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)

"""### Highlight face area"""

from PIL import Image, ImageFont, ImageDraw, ImageEnhance

def highlight_faces(pil_image, faces, outline_color="red"):
    """
    Highlight faces using pillow
    
    Args:
        pil_image (object): PiL image object
        faces (array): Numpy array
        outline_color (str, optional): Border color. Defaults to "red".
    
    Returns:
        [type]: [description]
    """
    draw = ImageDraw.Draw(pil_image)
    # for each face, draw a rectangle based on coordinates
    for face in faces:
        x, y, width, height = face['box']
        rect_start = (x, y)
        rect_end = ((x + width), (y + height))
        draw.rectangle((rect_start, rect_end), outline=outline_color)
    return pil_image

from matplotlib.pyplot import imshow

folder_random = os.path.join(FOLDER_FULL_IMAGES, random.choice(os.listdir(FOLDER_FULL_IMAGES)))
image_path = os.path.join(folder_random, random.choice(os.listdir(folder_random)))

image, pixels = read_image(image_path)
faces = detect_faces_in_image(pixels, detector)

img_highlighted_face = highlight_faces(image, faces)
imshow(asarray(img_highlighted_face))

"""## Recursively Iterate in Dir Extract and save faces from each image
---


*   Recursively iterate and extract face from file and write
*   keep directory stucture same
"""

src = os.path.join(dataset_folder, FOLDER_FULL_IMAGES)
dest = os.path.join(dataset_folder, FOLDER_FACE_ONLY)

# task  'extract_write_face' or 'face_alignment' (default: Copy file no change)
recursive_dir(src, dest, task="extract_write_face")

"""## Visualizing Data

### Plot all faces of single folder only
"""

from os import listdir
from PIL import Image
from numpy import asarray
from matplotlib import pyplot

# /content/gdrive/My Drive/deep_learning/vgg_2_dataset/face_only/train/n000706/0383_01.jpg
dir_face = os.path.join(dataset_folder, "full_images/")

# pick randome folder
folder = os.path.join(dir_face, random.choice(os.listdir(dir_face)))

i = 1
# enumerate files
for filename in listdir(folder):
    if i > 14:
        break
    # path
    image_path = os.path.join(folder, filename)
    image, pixels = read_image(image_path)
    # plot
    pyplot.subplot(2, 7, i)
    pyplot.axis('off')
    pyplot.imshow(pixels)
    i += 1
pyplot.show()



"""### Plot single image"""

image_path = os.path.join(dataset_folder, "full_images/n000452/0010_01.jpg")
image, pixels = read_image(image_path)
extracted_face = extract_faces_from_image(pixels, detector)

# Display the first face from the extracted faces
plt.imshow(extracted_face[0])
plt.show()

"""## Create Face Embeddings (128 d)"""

# calculate a face embedding for each face in the dataset using facenet
from numpy import load
from numpy import expand_dims
from numpy import asarray
from numpy import savez_compressed
from keras.models import load_model


def get_embedding(model, face_pixels):
    """
    get the face embedding for one face
    
    Args:
        model (object): Keras loaded model that extract features
        face_pixels (array): Image array in numpy format
    Returns:
        array: one dimensional feature vector
    """
    # scale pixel values
    face_pixels = face_pixels.astype('float32')
    # face_pixels.shape # (160, 160, 3)
    # standardize pixel values across channels (global)
    mean, std = face_pixels.mean(), face_pixels.std()
    face_pixels = (face_pixels - mean) / std
    # transform face into one sample
    samples = expand_dims(face_pixels, axis=0)
    # make prediction to get embedding
    yhat = model.predict(samples)
    # yhat[0].shape # (128,)
    return yhat[0]

# load the facenet model
facenet_model_file = os.path.join(root_folder, 'mtcnn_face_recognition/facenet_keras.h5')
facenet_model = load_model(facenet_model_file, compile=False)
print('Loaded Model')

# folder_random = os.path.join(FOLDER_FULL_IMAGES, random.choice(os.listdir(FOLDER_FULL_IMAGES)))
# image_path = os.path.join(folder_random, random.choice(os.listdir(folder_random)))

# pil_image, pixels = read_image(image_path)

# extracted_faces = extract_faces_from_image(pixels, detector, required_size=(160, 160))

# face_pixels = extracted_faces[0]
# embedding = get_embedding(facenet_model, face_pixels)

"""## generate face dataset and save in numpy"""

training_folder = os.path.join(dataset_folder, FOLDER_FACE_ONLY)

# load train dataset
X, y = load_dataset(training_folder)
print(f"X.shape : {X.shape}")
print(f"y.shape : {y.shape}")

# faces_dataset_file = os.path.join(dataset_folder, "output/faces-dataset.npz")

# # save arrays to +one file in compressed format
# # savez_compressed(faces_dataset_file, trainX, trainy, testX, testy)
# savez_compressed(faces_dataset_file, X, y)

final_X = []
for face_pixels in X:
    embedding = get_embedding(facenet_model, face_pixels)
    # print(f"embedding.shape : {}") # (128,) # 1D
    # print(f"embedding.reshape(-1, 1)") # (128, 1) # 2D
    final_X.append(embedding.reshape(-1, 1))

final_X = asarray(final_X)
print(f"final_X.shape : {final_X.shape}") # (720, 128, 1)

face_embeddings_file = os.path.join(dataset_folder, "output/faces-embeddings.npz")

# save arrays to +one file in compressed format
savez_compressed(face_embeddings_file, final_X, y)

"""## Loading and Preparing the Data; Feature Engineering"""

from numpy import load
from sklearn.model_selection import train_test_split

# load the face dataset
# face_dataset_file = os.path.join(dataset_folder, "output/faces-dataset.npz")
# data = load(face_dataset_file)

face_embeddings_file = os.path.join(dataset_folder, "output/faces-embeddings.npz")
data = load(face_embeddings_file)
X, y = data['arr_0'], data['arr_1']

trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.22)

print("Dataset loaded ")
print(f"trainX.shape : {trainX.shape}")
print(f"trainy.shape : {trainy.shape}")

# test dataset
print(f"testX.shape : {testX.shape}")
print(f"testy.shape : {testy.shape}")

"""### visualize data count per class"""

# import seaborn as sns # to make our plots pretty
# currently commented because after loading seaboarn matplotlib color not working properly
# g = sns.countplot(y_train)

# Plot examples from the dataset
plt.imshow(trainX[4])

"""### Encode the Labels

Machine learning algorithms can not work with categorical data directly, the data must be converted to numbers first. We call the process of representing categorical data as binary (0/1) vectors one hot encoding.

The process is as follows:
    
*   We make a table with a column for each category
*   the value in the column is either 1 (if it is) or 0 (if not)

For. Ex.

![Encode labels](https://drive.google.com/uc?id=1EV85hrAv439AJMmfyQX1Gwb_hcc3J2aq)
"""

# trainy labels are in  string format
# print(trainy) # array(['elton_john',  'madonna', ....]

labels = list(set(trainy))
print(labels) # ['elton_john', 'jerry_seinfeld', 'ben_afflek', 'mindy_kaling', 'madonna']

# count unique values inside a list
num_classes = len(labels) # num_classes = 5

from sklearn.preprocessing import LabelEncoder

# label encode targets
label_encoder = LabelEncoder()
label_encoder.fit(labels)

# save classes to file
file_label_encoder = os.path.join(dataset_folder, 'label-encoder-classes.npy')
np.save(file_label_encoder, label_encoder.classes_)

# loading saved file
# encoder = LabelEncoder()
# encoder.classes_ = numpy.load(file_label_encoder)

# transform train and test labels
trainy = label_encoder.transform(trainy)
testy = label_encoder.transform(testy)



"""### Normalising the data

You want to make sure that your data is in the same scale / form to increase the integrity of the data. We call this normalising the data. We want the data to be on a scale from 0 to 1.

If we would not Normalise our data, larger data points in the data can cause instability in our Neural Network.
"""

# trainX = trainX.astype('float32')
# testX = testX.astype('float32')

# # normalization
# trainX /= 255
# testX /= 255

# already normalized because it is embedding

"""## creating siamese model"""

from __future__ import absolute_import
from __future__ import print_function
import numpy as np

import random
from keras.models import Model
from keras.layers import Input, Flatten, Dense, Dropout, Lambda
from keras.optimizers import RMSprop
from keras import backend as K

import matplotlib.pyplot as plt # plotting

def euclidean_distance(vects):
    x, y = vects
    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)
    return K.sqrt(K.maximum(sum_square, K.epsilon()))


def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)


def contrastive_loss(y_true, y_pred):
    '''Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    '''
    margin = 1
    square_pred = K.square(y_pred)
    margin_square = K.square(K.maximum(margin - y_pred, 0))
    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)


def create_pairs(x, digit_indices):
    '''Positive and negative pair creation.
    Alternates between positive and negative pairs.
    '''
    pairs = []
    labels = []
    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1
    for d in range(num_classes):
        for i in range(n):
            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]
            pairs += [[x[z1], x[z2]]]
            inc = random.randrange(1, num_classes)
            dn = (d + inc) % num_classes
            z1, z2 = digit_indices[d][i], digit_indices[dn][i]
            pairs += [[x[z1], x[z2]]]
            labels += [1, 0]
    return np.array(pairs), np.array(labels)


def create_base_network(input_shape):
    '''Base network to be shared (eq. to feature extraction).
    '''
    input = Input(shape=input_shape)
    x = Flatten()(input)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(128, activation='relu')(x)
    return Model(input, x)


def compute_accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    pred = y_pred.ravel() < 0.5
    return np.mean(pred == y_true)


def accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))

"""### Reshape values
 convert 4d to 3d
"""

trainX.shape # (561, 128, 1)

"""## Generate positive and negative pairs"""

import numpy as np

input_shape = trainX[0].shape
input_shape # (128, 1)

# # # create training+test positive and negative pairs
digit_indices = [np.where(trainy == i)[0] for i in range(num_classes)]
# integer_mapping = [i for i, l in enumerate(list(label_encoder.classes_))]
# print(digit_indices)
print(f"type(digit_indices) : {type(digit_indices)}") # list
print(f"len(digit_indices) : {len(digit_indices)}") # 10
print(f"digit_indices[0][0] : {digit_indices[0][0]}")  # 3


tr_pairs, tr_y = create_pairs(trainX, digit_indices)

# type(tr_pairs) # ndarray
print(f"tr_pairs.shape : {tr_pairs.shape}")  # (584, 2, 28, 28)
print(f"tr_y.shape : {tr_y.shape}")  # (584,)

digit_indices = [np.where(testy == i)[0] for i in range(num_classes)]
te_pairs, te_y = create_pairs(testX, digit_indices)

print(f"tr_pairs.shape : {tr_pairs.shape}") # (584, 2, 128, 1)

# When you say arr2d[:, 0], you're saying give me the 0th index of all the rows in arr2d 
# (this is another way of saying give me the 0th column). 
print(f"tr_pairs[:, 0].shape : {tr_pairs[:, 0].shape}") # (584, 128, 1)
print(f"type(tr_pairs[:, 0]) : {type(tr_pairs[:, 0])}") # <class 'numpy.ndarray'>

print(f"tr_pairs[0, :].shape : {tr_pairs[0, :].shape}") # (2, 128, 1)
print(f"tr_pairs[0, 0].shape : {tr_pairs[0, 0].shape}") # (128, 1)

print(f"len(tr_pairs[:, 0]) : {len(tr_pairs[:, 0])}") # 584
print(f"len(tr_pairs[:, 1]) : {len(tr_pairs[:, 1])}") # 584

# import pdb; pdb.set_trace()

# network definition
base_network = create_base_network(input_shape)
# type(base_network) # <class 'keras.engine.training.Model'>

# Define the tensors for the two input images
input_a = Input(shape=input_shape)
input_b = Input(shape=input_shape)

# type(input_a) # <class 'tensorflow.python.framework.ops.Tensor'>

# Generate the encodings (feature vectors) for the two images
# because we re-use the same instance `base_network`,
# the weights of the network
# will be shared across the two branches
processed_a = base_network(input_a)
processed_b = base_network(input_b)

# Add a customized layer to compute the absolute difference between the encodings
L1_layer = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)
# pass below to ablove lambda function
distance = L1_layer([processed_a, processed_b])

model = Model([input_a, input_b], distance)

# train
rms = RMSprop()

# For custom metrics
model.compile(loss=contrastive_loss, optimizer=rms, metrics=['accuracy', accuracy])

# When you say arr2d[:, 0], you're saying give me the 0th index of all the rows in arr2d 
# (this is another way of saying give me the 0th column). 
# ex. tr_pairs[:, 0]

history = model.fit(
    [tr_pairs[:, 0], tr_pairs[:, 1]],
    tr_y,
    batch_size=128,
    epochs=20,
    validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),
    verbose=1
)

# compute final accuracy on training and test sets
y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])
tr_acc = compute_accuracy(tr_y, y_pred)
y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])
te_acc = compute_accuracy(te_y, y_pred)

print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))
print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))

"""## Save Model and label encoder

---
"""

file_keras_model = os.path.join(dataset_folder, "face-recognition-model.h5")
model.save(file_keras_model)  # creates a HDF5 file 'my_model.h5'

# load saved model
from keras.models import load_model

# If you load model only for prediction (without training), you need to set compile flag to False:
model2 = load_model(file_keras_model,compile=False)



####################################################
#############################################################################
"""## Compare two image and get distance"""

folder_random = os.path.join(FOLDER_FULL_IMAGES, random.choice(os.listdir(FOLDER_FULL_IMAGES)))
image_1_path = os.path.join(folder_random, random.choice(os.listdir(folder_random)))

folder_random = os.path.join(FOLDER_FULL_IMAGES, random.choice(os.listdir(FOLDER_FULL_IMAGES)))
image_2_path = os.path.join(folder_random, random.choice(os.listdir(folder_random)))

image_paths = [
    image_1_path,
    image_2_path
]

faces = []
face_embeddings = []
for image_path in image_paths:
    pil_image, pixels = read_image(image_path)

    extracted_faces = extract_faces_from_image(pixels, detector)
    extracted_face = extracted_faces[0]
    embedding = get_embedding(facenet_model, extracted_face)
    embedding_reshaped = embedding.reshape(-1, 1) # (128, 1)
    faces.append(extracted_face)
    face_embeddings.append(embedding_reshaped)

# Display the first face from the extracted faces
plt.imshow(faces[0])
plt.show()

# Display the first face from the extracted faces
plt.imshow(faces[1])
plt.show()

# convert to input format
face_1 = face_embeddings[0].reshape(1, 128, 1)
face_2 = face_embeddings[1].reshape(1, 128, 1)

y_pred = model.predict([face_1, face_2])
print(f"y_pred : {y_pred}")

matching_score = y_pred[0][0]
thresh = 0.4
if matching_score <= thresh:
    print('>Image is a Match (%.3f <= %.3f)' % (matching_score, thresh))
    is_matched = True
else:
    print('>Not matching (%.3f > %.3f)' % (matching_score, thresh))
    is_matched = False

print(model.summary())

"""## Model visualization

---
"""

from keras.utils import plot_model

"""###  graph of the model
This will plot a graph of the model and save it to a file:
"""

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# list all data in history
print(history.history.keys())

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()